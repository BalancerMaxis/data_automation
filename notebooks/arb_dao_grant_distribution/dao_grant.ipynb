{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Fetch pool snapshots from balancer subgraph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from bal_addresses import AddrBook\n",
    "\n",
    "from gql import Client\n",
    "from gql import gql\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "\n",
    "from notebooks import get_block_by_ts\n",
    "from notebooks.airdrop_arb_distribution.constants import BALANCER_GRAPH_URL\n",
    "from notebooks.airdrop_arb_distribution.constants import POOLS_SNAPSHOTS_QUERY\n",
    "\n",
    "# End date from timestamp\n",
    "end_date = datetime.fromtimestamp(1699504235)\n",
    "start_date = end_date - timedelta(days=14)\n",
    "start_ts = int(start_date.timestamp())\n",
    "end_ts = int(end_date.timestamp())\n",
    "address_book = AddrBook(\"arbitrum\")\n",
    "# Calculate end date approx block height\n",
    "target_block = get_block_by_ts(end_ts, chain=\"mainnet\")\n",
    "ARB_TOKEN = \"0x912ce59144191c1204e64559fe8253a0e49e6548\"\n",
    "\n",
    "\n",
    "# Fetch all the data from the balancer subgraph\n",
    "def make_gql_client(url: str) -> Optional[Client]:\n",
    "    transport = RequestsHTTPTransport(url=url, retries=3)\n",
    "    return Client(\n",
    "        transport=transport, fetch_schema_from_transport=True, execute_timeout=60\n",
    "    )\n",
    "\n",
    "\n",
    "def get_balancer_pool_snapshots() -> Optional[List[Dict]]:\n",
    "    client = make_gql_client(BALANCER_GRAPH_URL)\n",
    "    all_snapthots = []\n",
    "    limit = 100\n",
    "    offset = 0\n",
    "    while True:\n",
    "        result = client.execute(\n",
    "            gql(POOLS_SNAPSHOTS_QUERY.format(first=limit, skip=offset, start_ts=start_ts, end_ts=end_ts)))\n",
    "        all_snapthots.extend(result['poolSnapshots'])\n",
    "        offset += limit\n",
    "        if len(result['poolSnapshots']) < limit - 1:\n",
    "            break\n",
    "    # Need to group by pool address, since there are multiple snapshots per pool and we need to calculate \n",
    "    # difference between first and last snapshot\n",
    "    pools = defaultdict(list)\n",
    "    for snapshot in all_snapthots:\n",
    "        pools[snapshot['pool']['address']].append(snapshot.get('protocolFee', 0))\n",
    "    # Now calculate the difference between first and last snapshot\n",
    "    fees_snapshots = []\n",
    "    for pool_addr, snapshots in pools.items():\n",
    "        if len(snapshots) > 1:\n",
    "            # Convert to int with respect that there might be null values and string values\n",
    "            first_snapshot = float(snapshots[0]) if snapshots[0] else 0\n",
    "            last_snapshot = float(snapshots[-1]) if snapshots[-1] else 0\n",
    "            fee_collected = first_snapshot - last_snapshot\n",
    "            assert fee_collected >= 0, f\"Fee collected for pool {pool_addr} is negative\"\n",
    "            fees_snapshots.append({\n",
    "                'pool': {\n",
    "                    'address': pool_addr,\n",
    "                },\n",
    "                'protocolFee': fee_collected\n",
    "            })\n",
    "    return fees_snapshots\n",
    "\n",
    "\n",
    "pool_snapshots = get_balancer_pool_snapshots()\n",
    "print(f\"Collected data for dates: {start_date.date()} - {end_date.date()}\")\n",
    "print(f\"Block height at the end date: {target_block}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate BAL emissions per week:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from notebooks.arb_dao_grant_distribution.emissions_per_year import EMISSIONS_PER_YEAR\n",
    "from notebooks.arb_dao_grant_distribution.constants import CURRENT_YEAR\n",
    "\n",
    "emissions_per_week = 0\n",
    "for item in EMISSIONS_PER_YEAR['data']:\n",
    "    if item['year'] == str(CURRENT_YEAR):\n",
    "        emissions_per_week = float(item['balPerWeek'])\n",
    "        break\n",
    "print(f'Current BAL emissions per week: {emissions_per_week}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-process all the data in this cell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from notebooks import fetch_all_pools_info\n",
    "from notebooks.arb_dao_grant_distribution.static_boosts import STATIC_BOOST\n",
    "from notebooks.arb_dao_grant_distribution.constants import BALANCER_GAUGE_CONTROLLER_ABI\n",
    "from notebooks.arb_dao_grant_distribution.constants import BALANCER_GAUGE_CONTROLLER_ADDR\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from web3 import Web3\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "\n",
    "load_dotenv()\n",
    "ARBITRUM_CHAIN_LITERAL = \"ARBITRUM\"\n",
    "web3 = Web3(Web3.HTTPProvider(os.environ[\"ETHNODEURL\"]))\n",
    "\n",
    "# fetch balancer token usd price:\n",
    "cg = CoinGeckoAPI()\n",
    "bal_token_price = cg.get_price(ids='balancer', vs_currencies='usd')['balancer']['usd']\n",
    "# Fetch all pools from Balancer API\n",
    "all_pools = fetch_all_pools_info(\"arbitrum\")\n",
    "\n",
    "# Collect arb gauges\n",
    "arb_gauges = {}\n",
    "for pool in all_pools:\n",
    "    # Only collect gauges for the arb chain and that are not killed\n",
    "    if pool['chain'] == ARBITRUM_CHAIN_LITERAL and pool['gauge']['isKilled'] is False:\n",
    "        _gauge_addr = Web3.to_checksum_address(pool['gauge']['address'])\n",
    "        arb_gauges[_gauge_addr] = {\n",
    "            'gaugeAddress': pool['gauge']['address'],\n",
    "            'pool': pool['address'],\n",
    "            'symbol': pool['symbol'],\n",
    "            'id': pool['id'],\n",
    "        }\n",
    "\n",
    "gauge_c_contract = web3.eth.contract(address=BALANCER_GAUGE_CONTROLLER_ADDR, abi=BALANCER_GAUGE_CONTROLLER_ABI)\n",
    "\n",
    "boost_data = {}\n",
    "cap_override_data = {}\n",
    "# Load static boost here\n",
    "for boost in STATIC_BOOST:\n",
    "    _gauge_addr = Web3.to_checksum_address(boost['gaugeAddress'])\n",
    "    boost_data[_gauge_addr] = boost.get('fixedBoost', 1)\n",
    "    cap_override_data[_gauge_addr] = boost.get('capOverride', 10)\n",
    "pool_protocol_fees = {}\n",
    "\n",
    "# Collect protocol fees from the pool snapshots:\n",
    "for gauge_addr, gauge_data in arb_gauges.items():\n",
    "    for pool_snapshot in pool_snapshots:\n",
    "        if Web3.to_checksum_address(pool_snapshot['pool']['address']) == Web3.to_checksum_address(gauge_data['pool']):\n",
    "            # Since snapshots are sorted by timestamp descending, we can just take the first one we find for each pool and break\n",
    "            protocol_fee = float(pool_snapshot['protocolFee']) if pool_snapshot['protocolFee'] else 0\n",
    "            pool_protocol_fees[Web3.to_checksum_address(gauge_addr)] = protocol_fee\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply boost data to arb gauges\n",
    "vote_weights = {}\n",
    "combined_boost = {}\n",
    "# Dynamic boost data to print out in the final table\n",
    "dynamic_boosts = {}\n",
    "# Collect gauge voting weights from the gauge controller on chain\n",
    "for gauge_addr, gauge_data in arb_gauges.items():\n",
    "    weight = gauge_c_contract.functions.gauge_relative_weight(Web3.to_checksum_address(gauge_addr)).call(\n",
    "        block_identifier=target_block) / 1e18 * 100\n",
    "    arb_gauges[gauge_addr]['weightNoBoost'] = weight\n",
    "    # Calculate dynamic boost. Formula is `[Fees earned/value of bal emitted per pool + 1]`\n",
    "    dollar_value_of_bal_emitted = weight * emissions_per_week * bal_token_price\n",
    "    if dollar_value_of_bal_emitted != 0:\n",
    "        dynamic_boost = (pool_protocol_fees.get(gauge_addr, 0) / dollar_value_of_bal_emitted) + 1\n",
    "    else:\n",
    "        dynamic_boost = 1\n",
    "    dynamic_boosts[gauge_addr] = dynamic_boost\n",
    "    # Now calculate the final boost value, which uses formula - (dynamic boost + fixed boost) - 1\n",
    "    boost = (dynamic_boost + boost_data.get(gauge_addr, 1)) - 1\n",
    "    combined_boost[gauge_addr] = boost\n",
    "    weight *= boost\n",
    "    vote_weights[gauge_addr] = weight\n",
    "    arb_gauges[gauge_addr]['voteWeight'] = weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate arbitrum distribution across gauges"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from notebooks.arb_dao_grant_distribution.constants import ARBITRUM_TOTAL\n",
    "from notebooks.arb_dao_grant_distribution.constants import ARBITRUM_BONUS\n",
    "from notebooks.arb_dao_grant_distribution.constants import GAUGES_WITH_BONUSES\n",
    "from notebooks.arb_dao_grant_distribution.constants import VOTE_CAP_IN_PERCENT\n",
    "from notebooks.arb_dao_grant_distribution.constants import ARBITRUM_TO_DISTRIBUTE\n",
    "from notebooks import get_abi\n",
    "from IPython.core.display import HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Vote caps in percents are calculated as a percentage of the total amount of arb to distribute\n",
    "VOTE_CAPS_IN_PERCENTS = {gauge_addr: cap_override_data.get(gauge_addr, VOTE_CAP_IN_PERCENT) for gauge_addr in\n",
    "                         arb_gauges.keys()}\n",
    "# Custom gauge caps taken from boost data, calculated as a percentage of the total amount of arb to distribute\n",
    "VOTE_CAPS = {gauge_addr: VOTE_CAPS_IN_PERCENTS[gauge_addr] / 100 * ARBITRUM_TOTAL for gauge_addr in\n",
    "             arb_gauges.keys()}\n",
    "\n",
    "# Calculate total weight\n",
    "total_weight = sum([gauge['voteWeight'] for gauge in arb_gauges.values()])\n",
    "arb_gauge_distributions = {}\n",
    "for gauge_addr, gauge_data in arb_gauges.items():\n",
    "    gauge_addr = Web3.to_checksum_address(gauge_addr)\n",
    "    # Calculate distribution based on vote weight and total weight\n",
    "    to_distribute = ARBITRUM_TO_DISTRIBUTE * gauge_data['voteWeight'] / total_weight\n",
    "    # Cap distribution\n",
    "    to_distribute = to_distribute if to_distribute < VOTE_CAPS[gauge_addr] else VOTE_CAPS[gauge_addr]\n",
    "    # Get arb gauge addr\n",
    "    mainnet_arb_root_gauge_contract = web3.eth.contract(\n",
    "        address=Web3.to_checksum_address(gauge_addr),\n",
    "        abi=get_abi(\"ArbRootGauge\")\n",
    "    )\n",
    "    arb_gauge_distributions[gauge_addr] = {\n",
    "        'recipientGaugeAddr': mainnet_arb_root_gauge_contract.functions.getRecipient().call(),\n",
    "        'symbol': gauge_data['symbol'],\n",
    "        'voteWeight': gauge_data['voteWeight'],\n",
    "        'voteWeightNoBoost': gauge_data['weightNoBoost'],\n",
    "        'distribution': to_distribute if to_distribute < VOTE_CAPS[gauge_addr] else VOTE_CAPS[gauge_addr],\n",
    "        '%distribution': to_distribute / ARBITRUM_TOTAL * 100,\n",
    "        'boost': combined_boost.get(gauge_addr, 1),\n",
    "        'staticBoost': boost_data.get(gauge_addr, 1),\n",
    "        'dynamicBoost': dynamic_boosts.get(gauge_addr, 1),\n",
    "        'cap': f\"{cap_override_data.get(gauge_addr, VOTE_CAP_IN_PERCENT)}%\",\n",
    "        'bonus': 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Spend unspent arb on the gauges that are not capped yet\n",
    "def recur_distribute_unspend_arb():\n",
    "    unspent_arb = ARBITRUM_TO_DISTRIBUTE - sum([gauge['distribution'] for gauge in arb_gauge_distributions.values()])\n",
    "    if unspent_arb > 0:\n",
    "        # Find out total voting weight of uncapped gauges and mark it as 100%:\n",
    "        total_uncapped_weight = sum(\n",
    "            [g['voteWeight'] for g in [\n",
    "                gauge for addr, gauge in arb_gauge_distributions.items() if gauge['distribution'] < VOTE_CAPS[addr]]\n",
    "             ]\n",
    "        )\n",
    "        # Iterate over uncapped gauges and distribute unspent arb proportionally to their voting weight which is total uncapped weight\n",
    "        for a, uncap_gauge in {addr: gauge for addr, gauge in arb_gauge_distributions.items() if\n",
    "                               gauge['distribution'] < VOTE_CAPS[addr]}.items():\n",
    "            # For each loop calculate unspend arb\n",
    "            unspent_arb = ARBITRUM_TO_DISTRIBUTE - sum(\n",
    "                [gauge['distribution'] for gauge in arb_gauge_distributions.values()])\n",
    "            # Don't distribute more than vote cap\n",
    "            distribution = min(\n",
    "                uncap_gauge['distribution'] + unspent_arb * uncap_gauge['voteWeight'] / total_uncapped_weight,\n",
    "                VOTE_CAPS[a])\n",
    "            uncap_gauge['distribution'] = distribution\n",
    "            uncap_gauge['%distribution'] = uncap_gauge['distribution'] / ARBITRUM_TOTAL * 100\n",
    "    # Call recursively if there is still unspent arb\n",
    "    if ARBITRUM_TO_DISTRIBUTE - sum([g['distribution'] for g in arb_gauge_distributions.values()]) > 0:\n",
    "        recur_distribute_unspend_arb()\n",
    "\n",
    "\n",
    "recur_distribute_unspend_arb()\n",
    "\n",
    "print(\n",
    "    f\"Unspent arb: {ARBITRUM_TO_DISTRIBUTE - sum([gauge['distribution'] for gauge in arb_gauge_distributions.values()])}\")\n",
    "print(f\"Arb distributed: {sum([gauge['distribution'] for gauge in arb_gauge_distributions.values()])}\")\n",
    "# # Remove arb gauges with 0 distribution\n",
    "arb_gauge_distributions = {addr: gauge for addr, gauge in arb_gauge_distributions.items() if\n",
    "                           gauge['distribution'] > 0}\n",
    "# Toss in bonus arb to the predefined gauge:\n",
    "for gauge, gauge_info in GAUGES_WITH_BONUSES.items():\n",
    "    # Calculate bonus per pool\n",
    "    arbitrum_bonus_per_pool = ARBITRUM_BONUS / len(GAUGES_WITH_BONUSES)\n",
    "    # If gauge already exists, add bonus to it and recalculate % distribution, if not - create new gauge with bonus\n",
    "    if arb_gauge_distributions.get(gauge):\n",
    "        arb_gauge_distributions[gauge]['distribution'] += arbitrum_bonus_per_pool\n",
    "        arb_gauge_distributions[gauge]['%distribution'] = arb_gauge_distributions[gauge]['distribution'] / ARBITRUM_TOTAL * 100\n",
    "        arb_gauge_distributions[gauge]['bonus'] = arbitrum_bonus_per_pool\n",
    "    else:\n",
    "        arb_gauge_distributions[gauge] = {\n",
    "            'recipientGaugeAddr': gauge_info['recipientGauge'],\n",
    "            'symbol': gauge_info['symbol'],\n",
    "            'voteWeight': 0,\n",
    "            'voteWeightNoBoost': 0,\n",
    "            'distribution': arbitrum_bonus_per_pool,\n",
    "            '%distribution': arbitrum_bonus_per_pool / ARBITRUM_TOTAL * 100,\n",
    "            'boost': combined_boost.get(gauge, 1),\n",
    "            'staticBoost': boost_data.get(gauge, 1),\n",
    "            'dynamicBoost': dynamic_boosts.get(gauge, 1),\n",
    "            'cap': f\"{cap_override_data.get(gauge, VOTE_CAP_IN_PERCENT)}%\",\n",
    "            'bonus': arbitrum_bonus_per_pool\n",
    "        }\n",
    "arb_gauge_distributions_df = pd.DataFrame.from_dict(arb_gauge_distributions, orient='index')\n",
    "arb_gauge_distributions_df = arb_gauge_distributions_df.sort_values(by='%distribution', ascending=False)\n",
    "print(f\"Total arb distributed incl bonus: {sum([gauge['distribution'] for gauge in arb_gauge_distributions.values()])}\")\n",
    "display(HTML(arb_gauge_distributions_df.to_html(index=False)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display pie chart of distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = arb_gauge_distributions_df['symbol']\n",
    "sizes = arb_gauge_distributions_df['%distribution']\n",
    "explode = [0.1 if label == 'ARB/USD' else 0 for label in labels]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "# Increase chart size to 10x10\n",
    "fig1.set_size_inches(10, 10)\n",
    "ax1.set_title(f'Arbitrum airdrop distribution {start_date.date()} - {end_date.date()}')\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export to json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "arb_gauge_distributions_df.to_csv(f'./output/dao_grant_{start_date.date()}_{end_date.date()}.csv',\n",
    "                                  index=False)\n",
    "\n",
    "# Dump into output.json using:\n",
    "with open('../../data/output_tx_template.json') as f:\n",
    "    output_data = json.load(f)\n",
    "with open('../../data/transfer.json') as f:\n",
    "    json_transfer = json.load(f)\n",
    "with open('../../data/manualDeposit.json') as f:\n",
    "    json_manualDeposit = json.load(f)\n",
    "with open('../../data/setRecipientList.json') as f:\n",
    "    json_setRecipientList = json.load(f)\n",
    "    \n",
    "#### Handle setRecipientList\n",
    "gauge_distributions = arb_gauge_distributions.values()\n",
    "# Inject list of gauges addresses:\n",
    "json_setRecipientList[\"to\"] = address_book.extras.maxiKeepers.gaugeRewardsInjectors.arb_rewards_injector\n",
    "json_setRecipientList['contractInputsValues'][\n",
    "    'gaugeAddresses'] = f\"[{','.join([gauge['recipientGaugeAddr'] for gauge in gauge_distributions])}]\"\n",
    "# Inject vote weights:\n",
    "# Dividing by 2 since we are distributing for 2 weeks and 1 week is a period\n",
    "json_setRecipientList['contractInputsValues'][\n",
    "    'amountsPerPeriod'] = f\"[{','.join([str(int(gauge['distribution'] * 1e18 / 2)) for gauge in gauge_distributions])}]\"\n",
    "json_setRecipientList['contractInputsValues']['maxPeriods'] = f\"[{','.join(['2' for gauge in gauge_distributions])}]\"\n",
    "output_data[\"transactions\"].append(json_setRecipientList)\n",
    "\n",
    "### Handle Transfer\n",
    "json_transfer[\"to\"] = ARB_TOKEN\n",
    "json_transfer[\"contractInputsValues\"][\"to\"] = address_book.extras.maxiKeepers.gaugeRewardsInjectors.arb_rewards_injector\n",
    "json_transfer[\"contractInputsValues\"][\"amount\"] = str(int(ARBITRUM_TOTAL * 1e18))\n",
    "output_data[\"transactions\"].append(json_transfer)\n",
    "\n",
    "\n",
    "# Dump back to arb_distribution_for_msig.json\n",
    "with open(f'./output/dao_grant_{start_date.date()}_{end_date.date()}.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
